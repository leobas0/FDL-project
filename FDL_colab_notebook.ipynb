{"cells":[{"cell_type":"markdown","metadata":{"id":"tw4CGawr0iGt"},"source":["# Requirements, libraries"]},{"cell_type":"markdown","source":[],"metadata":{"id":"AGsCcyBW2BSe"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fOD5h9LgMeIL","executionInfo":{"status":"ok","timestamp":1707314048674,"user_tz":-60,"elapsed":36725,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}},"outputId":"a47d29ba-e93f-43cf-9cd1-196e87a1d686"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n","Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n","Collecting segmentation_models_pytorch\n","  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.16.0+cu121)\n","Collecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting timm==0.9.2 (from segmentation_models_pytorch)\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (9.4.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0+cu121)\n","Collecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n","  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.20.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=87f1c80e3db6483280a4adb43033f813a86cedfd04cf65273d2aef961f1ebef9\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=1f7e99691af9e2ec5c268aaef1bab420c051933a184e490e5a0ec659fe4f72f0\n","  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n","Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.3 timm-0.9.2\n"]}],"source":["!pip install ternausnet > /dev/null\n","!pip install pillow\n","!pip install torchsummary\n","!pip install segmentation_models_pytorch"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PbE3kaRUzuq2","executionInfo":{"status":"ok","timestamp":1707314062470,"user_tz":-60,"elapsed":13803,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["from collections import defaultdict\n","import copy\n","import random\n","import os\n","import pandas as pd\n","import numpy as np\n","import shutil\n","from urllib.request import urlretrieve\n","import albumentations as A\n","import albumentations.augmentations.functional as F\n","from albumentations.pytorch import ToTensorV2\n","import cv2\n","import matplotlib.pyplot as plt\n","import ternausnet.models\n","from tqdm import tqdm\n","import torch\n","import torchvision\n","import torch.backends.cudnn as cudnn\n","import torch.nn as nn\n","import torch.optim\n","import tensorflow as tf\n","from tensorflow.python.client import device_lib\n","from torch.utils.data import Dataset, DataLoader\n","from google.colab import drive\n","from PIL import Image\n","import torchsummary\n","\n","cudnn.benchmark = True"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6cROdU6cv-ZO","executionInfo":{"status":"ok","timestamp":1707314063065,"user_tz":-60,"elapsed":601,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}},"outputId":"552a0e45-21ea-498b-ac00-7c6ac19758b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}],"source":["# Check what version of Python we are running\n","!python3 -V"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PnSAIX4fwCQG","executionInfo":{"status":"ok","timestamp":1707314063674,"user_tz":-60,"elapsed":614,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}},"outputId":"43a040bf-d39d-4d0f-af09-48cea82f5188"},"outputs":[{"output_type":"stream","name":"stdout","text":["x86_64\n"]}],"source":["!uname -m"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BXN81t6RHy0Y","outputId":"00199467-7337-4f38-eb56-bc8f5cec00e2","executionInfo":{"status":"ok","timestamp":1707314063674,"user_tz":-60,"elapsed":5,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/device:GPU:0\n"]},{"output_type":"execute_result","data":{"text/plain":["[name: \"/device:CPU:0\"\n"," device_type: \"CPU\"\n"," memory_limit: 268435456\n"," locality {\n"," }\n"," incarnation: 5120830603811891386\n"," xla_global_id: -1,\n"," name: \"/device:GPU:0\"\n"," device_type: \"GPU\"\n"," memory_limit: 14626652160\n"," locality {\n","   bus_id: 1\n","   links {\n","   }\n"," }\n"," incarnation: 16341653670883602795\n"," physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n"," xla_global_id: 416903419]"]},"metadata":{},"execution_count":5}],"source":["# check whether GPU is active\n","print(tf.test.gpu_device_name())\n","device_lib.list_local_devices()"]},{"cell_type":"markdown","metadata":{"id":"jJxiuYV_0jHy"},"source":["# Obtaining the data"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"vmMfEs8QxoU5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707314234143,"user_tz":-60,"elapsed":170473,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}},"outputId":"2747db4c-781a-424f-9909-b015f3c49f09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive = drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"DBb72dbgbqMv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707314239691,"user_tz":-60,"elapsed":5555,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}},"outputId":"89ff846b-8b91-4fa0-c3f4-96029e983c10"},"outputs":[{"output_type":"stream","name":"stdout","text":["374\n","75\n","299\n"]}],"source":["# Define the path for raw folder and masks folder\n","#set who to baso or leo\n","who = \"baso\"\n","\n","if (who == \"leo\"):\n","  masks_folder_path = '/content/drive/MyDrive/deep_learning/kaggl/vectors/random-split-_2022_11_17-22_35_45/Masks'\n","  raw_folder_path = '/content/drive/MyDrive/deep_learning/kaggl/rasters/raw'\n","  path_pred = '/content/drive/MyDrive/deep_learning/kaggl/rasters/predictions'\n","elif(who == \"baso\"):\n","  masks_folder_path = '/content/drive/MyDrive/Hurricane_Harvey/vectors/random-split-_2022_11_17-22_35_45/Masks'\n","  raw_folder_path = '/content/drive/MyDrive/Hurricane_Harvey/rasters/raw'\n","  prediction_folder = '/content/drive/MyDrive/deep_learning/kaggl/rasters/raw_prediction'\n","  path_pred = '/content/drive/MyDrive/Hurricane_Harvey/predictions2'\n","\n","\n","# Create an empty list to store the results\n","test_pics=[]\n","train_pics=[]\n","\n","# Get a list of all files in folder B\n","files_b = os.listdir(raw_folder_path)\n","\n","# Iterate through the files in folder B\n","for file_b in files_b:\n","    # Get the file name without the file format\n","    file_name, file_extension = os.path.splitext(file_b)\n","    # Check if a file with the same name and .png extension exists in folder A\n","    if os.path.isfile(os.path.join(masks_folder_path, file_name + '.png')):\n","        train_pics.append(file_name)\n","    else:\n","        test_pics.append(file_name)\n","print(len(files_b))\n","print(len(test_pics))\n","print(len(train_pics))"]},{"cell_type":"markdown","metadata":{"id":"vXGS--ME1lz0"},"source":["Once the raw images are stored in one place, we create a variable to iterate through them. To do so, we generate a list, sort the list, and then shuffle the list. This helps us ensure that there is no entry bias.  We also check the amount of pictures we have, to make sure that no mistakes were made."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"9qF-k4Lfg_RX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707314239691,"user_tz":-60,"elapsed":21,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}},"outputId":"359fd333-dff6-4f3d-a53e-543361c793a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["In total, there are 374 images available to work with 374\n"]}],"source":["# Create a list of raw images in tif format\n","images_filenames = list(sorted(os.listdir(raw_folder_path)))\n","print(\"In total, there are 374 images available to work with \"+str(len(images_filenames)))\n","\n","random.seed(42)\n","random.shuffle(images_filenames)"]},{"cell_type":"markdown","metadata":{"id":"YE0UCYJR5vc8"},"source":["We keep 20% of our training set for validation purposes."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"8Oc4lQUm6RgQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707314239692,"user_tz":-60,"elapsed":19,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}},"outputId":"961c2641-aac4-4f74-d173-c4e87c9345da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Our train set contains 240 images.\n","Our validation set contains 58 images.\n"]}],"source":["#splitting training data in train_set (80%) and val_set (20%)\n","#random.shuffle(train_pics)\n","train_set = train_pics[0:240]\n","print(\"Our train set contains \"+str(len(train_set))+\" images.\")\n","\n","val_set = train_pics[241:300]\n","print(\"Our validation set contains \"+str(len(val_set))+\" images.\")"]},{"cell_type":"markdown","metadata":{"id":"I5M3fh5QdFLT"},"source":["We then create a function called \"display_image_grid\" that takes in several arguments: \"images_filenames\", \"images_directory\", \"masks_directory\", \"predicted_masks\", \"batch_size\", and \"size\". The function creates a grid of images using the matplotlib library and displays them using the OpenCV library.\n","\n","The function starts by defining the number of columns in the grid, which is 2 if \"predicted_masks\" is not provided, and 3 if it is. It then sets up a loop to iterate through the images in \"images_filenames\" in batches of size \"batch_size\". For each batch, it creates a new figure with a specific number of rows and columns using the \"plt.subplots()\" function.\n","\n","It then iterates through the images in the current batch and reads the image and mask files using the OpenCV library. It then resizes the image and mask to the given \"size\" using the \"cv2.resize()\" function. It then uses the \"imshow()\" function from OpenCV to display the images in the grid. The first column is the original image, the second column is the ground-truth mask, and the third column is the predicted mask (if provided). The function also includes axis labels and titles for each column. Finally, it shows the grid using \"plt.show()\" and pauses for 0.5 seconds before closing the plot and moving on to the next batch of images."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"TF53jzhFKhBw","executionInfo":{"status":"ok","timestamp":1707314239692,"user_tz":-60,"elapsed":10,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["def display_image_grid(images_filenames, images_directory, masks_directory, predicted_masks=None, batch_size=10, size=(256,256)):\n","    cols = 3 if predicted_masks else 2\n","    rows = len(images_filenames)\n","    k = 0\n","    # Create a figure with a specific number of rows and columns\n","    #figure, ax = plt.subplots(nrows=batch_size, ncols=cols, figsize=(10, 24))\n","\n","    for i in range(0,rows,batch_size):\n","        figure, ax = plt.subplots(nrows=batch_size, ncols=cols, figsize=(10, 24))\n","        for j, image_filename in enumerate(images_filenames[i:i+batch_size]):\n","            k = k + 1\n","            image_path = (os.path.join(images_directory, image_filename) + \".tif\")\n","            mask_path = (os.path.join(masks_directory, image_filename) + \".png\")\n","\n","            # Read the image and reduce its size\n","            image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n","            image = cv2.cvtColor(cv2.resize(image,size), cv2.COLOR_BGR2RGB)\n","\n","            # Read the mask and reduce its size\n","            mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n","            mask = cv2.resize(mask,size)\n","\n","            # Use the imshow() function from OpenCV to display the images\n","            ax[j, 0].imshow(image)\n","            ax[j, 1].imshow(mask, interpolation=\"nearest\")\n","\n","            ax[j, 0].set_title(\"Image\")\n","            ax[j, 1].set_title(\"Ground truth mask\")\n","\n","            ax[j, 0].set_axis_off()\n","            ax[j, 1].set_axis_off()\n","\n","            if predicted_masks:\n","                predicted_mask = predicted_masks[i+j]\n","                ax[j, 2].imshow(predicted_mask, interpolation=\"nearest\")\n","                ax[j, 2].set_title(\"Predicted mask\")\n","                ax[j, 2].set_axis_off()\n","        plt.tight_layout()\n","        plt.show(block=False)\n","        plt.pause(0.5)\n","        plt.close()\n","    print(\"number of plots:\" + str(k))"]},{"cell_type":"markdown","metadata":{"id":"v7R7YeQr4O67"},"source":["WARNING: The function below shows the train pics and their associated masks. It outputs close to 300 images and their masks in parallel. As so, please consider that running the function below will take a lot of space in memory.  "]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1kIhM72wirySQ9BajMAuG7ZFBfK0RiVI-"},"id":"lOklt6ByRlNn","outputId":"97b4a67a-f61d-4d63-cbe8-88c8704c181d","executionInfo":{"status":"ok","timestamp":1707314701470,"user_tz":-60,"elapsed":461788,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["print(len(train_pics))\n","\n","display_image_grid(train_pics, raw_folder_path, masks_folder_path)"]},{"cell_type":"markdown","metadata":{"id":"TX_iCUtkdpkG"},"source":["Here we create a custom dataset class that inherits from the PyTorch \"Dataset\" class. This class is here to load and preprocess images and masks that are specific to the Hurricane Harvey dataset.\n","\n","The class has a constructor \"init\" which takes in three arguments: \"images_filenames\", \"images_directory\", \"masks_directory\" and \"transform\". \"images_filenames\" is a list of image file names, \"images_directory\" is the directory where the images are located, \"masks_directory\" is the directory where the masks are located, and \"transform\" is an optional argument that can be used to apply data augmentation to the images and masks.\n","\n","\"len\" and \"getitem\" are required by PyTorch's dataset framework. \"len\" returns the number of images in the dataset and \"getitem\" is used to access a specific image and its corresponding mask.\n","\n","The \"getitem\" method takes the index of the image and reads the image using OpenCV library. It then converts the image from BGR to RGB color space. It also reads the corresponding mask file using OpenCV and converts it from 8-bit integer to float32. Finally, it applies transformations to the image and mask, if there is a mask provided.\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"798GBpUohspc","executionInfo":{"status":"ok","timestamp":1707314701476,"user_tz":-60,"elapsed":224,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["class Hurricane_Harvey(Dataset):\n","    def __init__(self, images_filenames, images_directory, masks_directory, transform=None):\n","        self.images_filenames = images_filenames\n","        self.images_directory = images_directory\n","        self.masks_directory = masks_directory\n","        self.transform = transform\n","\n","    # Returns the length of some set\n","    def __len__(self):\n","        return len(self.images_filenames)\n","\n","    # This accesses a specific images/mask\n","    def __getitem__(self, idx):\n","        image_filename = self.images_filenames[idx]\n","        image_path = (os.path.join(self.images_directory, image_filename) + \".tif\")\n","        #print(image_path)\n","        image = cv2.imread(image_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        mask_path = (os.path.join(self.masks_directory, image_filename) + \".png\")\n","        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE,)\n","        #mask = mask.astype(np.uint8)\n","        mask = mask.astype(np.float32)\n","        #print(mask)\n","        if self.transform is not None:\n","            transformed = self.transform(image=image, mask=mask)\n","            image = transformed[\"image\"]\n","            mask = transformed[\"mask\"]\n","        return image, mask"]},{"cell_type":"markdown","metadata":{"id":"DqE_pL1ieKiN"},"source":["This code is defining two sets of image preprocessing steps, one for training data (train_transform) and one for validation data (val_transform). The training data preprocessing includes several image augmentation techniques such as resizing, shifting, rotating, color shifting, and adjusting brightness and contrast. These techniques are applied randomly with a probability of 0.5 for each step. The final step in the training preprocessing is normalizing the image by subtracting mean values and dividing by standard deviations and converting the image to a PyTorch tensor using ToTensorV2.\n","\n","The validation data preprocessing is less extensive, it only includes resizing and normalizing the image by subtracting mean values and dividing by standard deviations and converting the image to a PyTorch tensor using ToTensorV2.\n","\n","The train_dataset and val_dataset are created by using the Hurricane_Harvey class which is custom dataset class and passing the respective set of images(train_set, val_set), the raw image folder path, mask folder path and the respective transform functions."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"gJpNL63GeXsh","executionInfo":{"status":"ok","timestamp":1707314701478,"user_tz":-60,"elapsed":206,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["train_transform = A.Compose(\n","    [\n","        A.Resize(480, 640), #resize images to 480*640 pixels\n","        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5), #randomly shift, scale and rotate images\n","        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5), #randomly change colors\n","        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5), #adjust brightness and contrast\n","        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), #Normalize pixel values\n","        ToTensorV2(), #convert to pytorch tensor\n","    ]\n",")\n","\n","train_dataset = Hurricane_Harvey(train_set, raw_folder_path, masks_folder_path, transform=train_transform,)\n","\n","val_transform = A.Compose(\n","    [A.Resize(480, 640), A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()]\n",")\n","val_dataset = Hurricane_Harvey(val_set, raw_folder_path, masks_folder_path, transform=val_transform,)"]},{"cell_type":"markdown","metadata":{"id":"4Iri97GMejZN"},"source":["This code defines a function called \"visualize_augmentations\" which takes in three inputs: a dataset, an index of an image in the dataset, and the number of samples. The function creates a subplot with the specified number of rows and columns, and plots the original and augmented images using the Matplotlib library.\n","\n","The first line of the function makes a copy of the input dataset and removes the normalization and conversion to tensor steps from the dataset's transform attribute. This is done so that the visualized images will be in their original format and not normalized.\n","\n","The for loop iterates for the specified number of samples. In each iteration, it selects the image and mask from the dataset based on the specified index and plots the image and mask using the Matplotlib's imshow method. The titles of the images and masks are also set, and the axis is turned off.\n","\n","The function then shows the plot using the show method of Matplotlib and tightens the layout of the plot.\n","It allows to visualize the image and mask with the augmentations applied to them and is useful for debugging, quality control and understanding the effect of the applied augmentations."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"djeho-SVj3ZZ","executionInfo":{"status":"ok","timestamp":1707314701479,"user_tz":-60,"elapsed":203,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["def visualize_augmentations(dataset, idx=0, samples=5):\n","    dataset = copy.deepcopy(dataset)\n","    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n","    figure, ax = plt.subplots(nrows=samples, ncols=2, figsize=(10, 24))\n","    for i in range(samples):\n","        image, mask = dataset[idx]\n","        ax[i, 0].imshow(image)\n","        ax[i, 1].imshow(mask, interpolation=\"nearest\")\n","        ax[i, 0].set_title(\"Augmented image\")\n","        ax[i, 1].set_title(\"Augmented mask\")\n","        ax[i, 0].set_axis_off()\n","        ax[i, 1].set_axis_off()\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1RfrEBWo6LK2s55ndwRihxiDCnZcGSCAQ"},"id":"fHXPjCtuj7IF","outputId":"e69de6c5-387b-4465-9571-0b867a5ce989","executionInfo":{"status":"ok","timestamp":1707314701479,"user_tz":-60,"elapsed":201,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["random.seed(42) # the images are in random order\n","visualize_augmentations(train_dataset, idx=55)"]},{"cell_type":"markdown","metadata":{"id":"996mCNm_hu0Z"},"source":["In the train function, the images and target variables are used as inputs to the model and the criterion function.\n","\n","images: The images variable is used to store a batch of images from the train_loader, which is passed to the model. These images are used as input to the model to make predictions.\n","\n","target: The target variable is used to store the corresponding ground-truth labels or segmentation masks for the images. These are used as the correct output for the model, and the criterion function uses these to calculate the loss between the model's predictions and the target values.\n","\n","In this code, the images and targets are first moved to the appropriate device using the to method, based on the device specified in the params dictionary.\n","The model is then applied to the images, and the output is obtained. The criterion function is applied to the output and the target, which calculates the loss between the model's predictions and the correct outputs. This loss is then used to update the model's parameters using the optimizer.\n","\n","In summary, the images and target variables are used to train the model, the model takes images as input, and the criterion function uses the target as the correct output to calculate the loss and update the model's parameters."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"QI6g8kdVqMTz","executionInfo":{"status":"ok","timestamp":1707314701481,"user_tz":-60,"elapsed":175,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["params = {\n","    \"lr\": 0.001,\n","    \"batch_size\": 4,\n","    \"num_workers\": 1,\n","    \"epochs\": 10,\n","}"]},{"cell_type":"markdown","metadata":{"id":"IBm_aTTRBKG1"},"source":["\n","\n","```\n","```\n","\n","# Running the model"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":169,"status":"ok","timestamp":1707314701482,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"},"user_tz":-60},"id":"dGLpKroOqoWw","outputId":"1804ae38-187b-46cd-aa7a-7b79cd2ed6ef"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n","100%|██████████| 13.6M/13.6M [00:00<00:00, 24.7MB/s]\n"]}],"source":["#first submission DeepLabV3 mobilenet_v2 Score: 72.24\n","#params = {\n","#    \"lr\": 0.001,\n","#    \"batch_size\": 4,\n","#    \"num_workers\": 1,\n","#    \"epochs\": 10,\n","#}\n","\n","import segmentation_models_pytorch as smp\n","\n","model = smp.DeepLabV3('mobilenet_v2', encoder_weights='imagenet',\n","                 in_channels=3,\n","                 classes=27, activation=None,\n","                 encoder_depth=5, decoder_channels=512)\n","\n","#max_lr = 1e-3\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","weight_decay = 1e-4\n","patience = 5\n","criterion = nn.CrossEntropyLoss().to(device)\n","dice_loss = smp.losses.DiceLoss(mode='multiclass', classes=27)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=params[\"lr\"], weight_decay=weight_decay)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n","#device = params['device']"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":723,"status":"ok","timestamp":1707314702096,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"},"user_tz":-60},"id":"Djnmfi5Z4dKJ","outputId":"db4f9f85-bc8b-4bf1-f913-d98e86a0e3ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.10/dist-packages (0.3.3)\n","Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.16.0+cu121)\n","Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.7.4)\n","Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.7.1)\n","Requirement already satisfied: timm==0.9.2 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.9.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (9.4.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.0+cu121)\n","Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.20.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n","100%|██████████| 83.3M/83.3M [00:00<00:00, 223MB/s]\n"]}],"source":["!pip install segmentation-models-pytorch\n","\n","#first PSPNet resnet101 submission  Score: 70.32\n","#params = {\n","    #\"lr\": 0.001,\n","    #\"batch_size\": 4,\n","    #\"num_workers\": 1,\n","    #\"epochs\": 10,\n","#}\n","\n","#second PSPNet resnet101 submission Score: 14.39\n","#\n","#params = {\n","#    \"lr\": 0.0001,\n","#    \"batch_size\": 4,\n","#    \"num_workers\": 1,\n","#    \"epochs\": 20,\n","#}\n","\n","#third PSPNet resnet34 submission Score: 70.25\n","\n","#params = {\n","#    \"lr\": 0.001,\n","#    \"batch_size\": 4,\n","#    \"num_workers\": 1,\n","#    \"epochs\": 5,\n","#}\n","\n","#fourth PSPNet resnet34 submission Score: 66.06\n","\n","#params = {\n","    #\"lr\":  0.000001,\n","    #\"batch_size\": 4,\n","    #\"num_workers\": 1,\n","    #\"epochs\": 25,\n","#}\n","\n","#fifth PSPNet resnet34 submission Score: 70.63\n","\n","#params = {\n","#    \"lr\": 0.001,\n","#    \"batch_size\": 4,\n","#    \"num_workers\": 1,\n","#    \"epochs\": 10,\n","#}\n","\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch import PSPNet\n","\n","\n","model = smp.PSPNet(\n","    encoder_name = 'resnet34',\n","    encoder_weights = 'imagenet',\n","    classes = 27,\n","    activation = 'softmax2d', # could be None for logits or 'softmax2d' for multiclass segmentation\n",")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","weight_decay = 1e-4\n","patience = 5\n","criterion = nn.CrossEntropyLoss().to(device)\n","dice_loss = smp.losses.DiceLoss(mode='multiclass', classes=27)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=params[\"lr\"], weight_decay=weight_decay)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n","#"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"uWWpJqPFtxGu","executionInfo":{"status":"ok","timestamp":1707314702102,"user_tz":-60,"elapsed":58,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["def dataloaders(model, train_dataset, val_dataset, params):\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=params[\"batch_size\"],\n","        shuffle=True,\n","        num_workers=params[\"num_workers\"],\n","        pin_memory=True,\n","    )\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=params[\"batch_size\"],\n","        shuffle=False,\n","        num_workers=params[\"num_workers\"],\n","        pin_memory=True,\n","    )\n","    return train_loader, val_loader\n","\n","train_loader, val_loader = dataloaders(model, train_dataset, val_dataset, params)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"t9D3ysGTlSbt","executionInfo":{"status":"ok","timestamp":1707316224488,"user_tz":-60,"elapsed":658,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["from tqdm import tqdm\n","\n","import time\n","\n","import torch.nn.functional as F\n","\n","dice_loss = smp.losses.DiceLoss(mode='multiclass', classes=27)\n","\n","def get_accuracy(output, mask):\n","    with torch.no_grad():\n","        output = torch.argmax(F.softmax(output,dim=1), dim=1)\n","        correct = torch.eq(output, mask).int()\n","        accuracy = float(correct.sum()) / float(correct.numel())\n","    return accuracy\n","\n","def fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patience):\n","    torch.cuda.empty_cache()\n","    total_train_loss = []\n","    total_val_loss = []\n","    train_acc_log = []\n","    train_dice_log = []\n","    train_loss_log = []\n","    val_acc_log = []\n","    val_loss_log = []\n","    lrs = []\n","    best_val_dice = 0\n","    train_dice_tot = 0\n","    val_dice_tot = 0\n","    early_stopping_counter = 0\n","\n","    model.to(device)\n","    for e in range(epochs):\n","        print(f'epoch: {e}')\n","        train_loss = []\n","        running_loss = 0\n","        accuracy = 0\n","        train_dice = []\n","        model.train()\n","        for i, batch in enumerate(tqdm(train_loader)):\n","            #training phase\n","            optimizer.zero_grad()\n","            images, masks = batch\n","            images, masks = images.to(device), masks.to(device)\n","            masks = masks.type(torch.LongTensor)\n","            output = model(images.to(device))\n","            loss = criterion(output.float().to(device), masks.to(device))\n","            dice = dice_loss(output.float().to(device), masks.to(device))\n","            dice_s = 1 - dice.item()\n","            accuracy += get_accuracy(output.float().to(device), masks.to(device))\n","\n","            # wandb.log({\"train_loss\":loss.item()})\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","            #lrs.append(get_lr(optimizer))\n","            train_dice.append(dice_s)\n","\n","            running_loss += loss.item()\n","            train_loss.append(loss.item())\n","            #scheduler.step()\n","\n","        train_acc = (accuracy / len(train_loader))\n","        train_dice_tot = np.mean(train_dice)\n","        train_acc_log.append(train_acc)\n","        train_loss_mean = np.mean(train_loss)\n","        train_loss_log.append(train_loss_mean)\n","\n","\n","        model.eval()\n","        val_losses = []\n","        running_val_loss = 0\n","        val_accuracy = 0\n","        val_dice = []\n","        #validation loop\n","        with torch.no_grad():\n","            for i, val_batch in enumerate(tqdm(val_loader)):\n","                images, masks = val_batch\n","                images, masks = images.to(device), masks.to(device)\n","                masks = masks.type(torch.LongTensor)\n","                output = model(images.to(device))\n","                loss = criterion(output.float().to(device), masks.to(device))\n","                dice = dice_loss(output.float().to(device), masks.to(device))\n","                dice_s = 1 - dice.item()\n","                val_dice.append(dice_s)\n","                val_accuracy += get_accuracy(output.float().to(device), masks.to(device))\n","                running_val_loss += loss.item()\n","                val_losses.append(loss.item())\n","\n","        val_acc = (val_accuracy / len(val_loader))\n","        val_acc_log.append(val_acc)\n","        val_dice_tot = np.mean(val_dice)\n","        val_loss_mean = np.mean(val_losses)\n","        val_loss_log.append(val_loss_mean)\n","        # wandb.log({\"train_acc\":train_acc, \"val_acc\":val_acc, \"val_loss\":val_loss_mean})\n","\n","        print(f'Train Accuracy: {train_acc}\\nTrain Dice: {train_dice_tot}\\nTrain Loss: {train_loss_mean}\\nValidation Accuracy: {val_acc}\\nValidation Dice: {val_dice_tot}\\nValidation Loss: {val_loss_mean}')\n","        log = {\"train_acc\": train_acc_log, \"val_acc\": val_acc_log, \"train_loss\": train_loss_log, \"val_loss\": val_loss_log}\n","\n","        # Save the model if it has improved\n","        if val_dice_tot > best_val_dice:\n","            best_val_dice = val_dice_tot\n","            torch.save(model.state_dict(), 'model_APT_ss.pt')\n","            torch.save(model, 'model_APT.pt') # add name of model # add name of model\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","\n","        # If the counter reaches the patience, stop training\n","        if early_stopping_counter >= patience:\n","            print(\"Early stopping\")\n","            break\n","\n","        torch.cuda.empty_cache()\n","\n","    log = {\"train_acc\": train_acc_log, \"val_acc\": val_acc_log, \"train_loss\": train_loss_log, \"val_loss\": val_loss_log}\n","\n","    return log"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":511860,"status":"ok","timestamp":1707317228272,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"},"user_tz":-60},"id":"Ef_DpSVolUQD","outputId":"b8b68da5-b6ed-4f12-f6c5-5cca7562886c"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 0\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 60/60 [01:22<00:00,  1.38s/it]\n","100%|██████████| 15/15 [00:19<00:00,  1.28s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Accuracy: 0.591899210611979\n","Train Dice: 0.41945629169543586\n","Train Loss: 2.7657034595807395\n","Validation Accuracy: 0.5063196072048611\n","Validation Dice: 0.43837661743164064\n","Validation Loss: 2.8513951460520426\n","epoch: 1\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 60/60 [01:21<00:00,  1.35s/it]\n","100%|██████████| 15/15 [00:18<00:00,  1.24s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Accuracy: 0.5948851047092014\n","Train Dice: 0.42120857536792755\n","Train Loss: 2.762599309285482\n","Validation Accuracy: 0.5380431315104167\n","Validation Dice: 0.43880427479743955\n","Validation Loss: 2.819278939565023\n","epoch: 2\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 60/60 [01:20<00:00,  1.35s/it]\n","100%|██████████| 15/15 [00:18<00:00,  1.26s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Accuracy: 0.5982889675564234\n","Train Dice: 0.42548670222361884\n","Train Loss: 2.7591700037320455\n","Validation Accuracy: 0.5442037217881945\n","Validation Dice: 0.4388270338376363\n","Validation Loss: 2.813003746668498\n","epoch: 3\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 60/60 [01:19<00:00,  1.33s/it]\n","100%|██████████| 15/15 [00:18<00:00,  1.21s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train Accuracy: 0.5968086344401041\n","Train Dice: 0.42047981470823287\n","Train Loss: 2.760571996370951\n","Validation Accuracy: 0.5087409396701388\n","Validation Dice: 0.43825201590855917\n","Validation Loss: 2.8490739186604817\n","epoch: 4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 60/60 [01:20<00:00,  1.35s/it]\n","100%|██████████| 15/15 [00:18<00:00,  1.22s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Accuracy: 0.5957051730685765\n","Train Dice: 0.4136156365275383\n","Train Loss: 2.7619816144307454\n","Validation Accuracy: 0.5404442816840278\n","Validation Dice: 0.4389072914918264\n","Validation Loss: 2.817236057917277\n","epoch: 5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 60/60 [01:20<00:00,  1.34s/it]\n","100%|██████████| 15/15 [00:18<00:00,  1.21s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Accuracy: 0.6103218044704861\n","Train Dice: 0.42059609244267143\n","Train Loss: 2.747318454583486\n","Validation Accuracy: 0.5672310112847222\n","Validation Dice: 0.4397695819536845\n","Validation Loss: 2.78973077138265\n","epoch: 6\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 60/60 [01:19<00:00,  1.33s/it]\n","100%|██████████| 15/15 [00:19<00:00,  1.28s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Accuracy: 0.6330261773003473\n","Train Dice: 0.42743452191352843\n","Train Loss: 2.724489180246989\n","Validation Accuracy: 0.5532520616319444\n","Validation Dice: 0.43922861814498904\n","Validation Loss: 2.8040228048960367\n","epoch: 7\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 60/60 [01:20<00:00,  1.34s/it]\n","100%|██████████| 15/15 [00:18<00:00,  1.21s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Accuracy: 0.6170832655164928\n","Train Dice: 0.4117747520407041\n","Train Loss: 2.7403605540593463\n","Validation Accuracy: 0.5749059244791667\n","Validation Dice: 0.4397980550924937\n","Validation Loss: 2.782199827829997\n","epoch: 8\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 60/60 [01:20<00:00,  1.35s/it]\n","100%|██████████| 15/15 [00:18<00:00,  1.22s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Accuracy: 0.6361864827473956\n","Train Dice: 0.41779668380816776\n","Train Loss: 2.721375068028768\n","Validation Accuracy: 0.5778255208333333\n","Validation Dice: 0.43998106718063357\n","Validation Loss: 2.7795522212982178\n","epoch: 9\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 60/60 [01:19<00:00,  1.33s/it]\n","100%|██████████| 15/15 [00:19<00:00,  1.27s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Accuracy: 0.6414598524305555\n","Train Dice: 0.4208909715215365\n","Train Loss: 2.716049305597941\n","Validation Accuracy: 0.5855107421875\n","Validation Dice: 0.4400945941607157\n","Validation Loss: 2.7717891693115235\n"]},{"output_type":"execute_result","data":{"text/plain":["{'train_acc': [0.591899210611979,\n","  0.5948851047092014,\n","  0.5982889675564234,\n","  0.5968086344401041,\n","  0.5957051730685765,\n","  0.6103218044704861,\n","  0.6330261773003473,\n","  0.6170832655164928,\n","  0.6361864827473956,\n","  0.6414598524305555],\n"," 'val_acc': [0.5063196072048611,\n","  0.5380431315104167,\n","  0.5442037217881945,\n","  0.5087409396701388,\n","  0.5404442816840278,\n","  0.5672310112847222,\n","  0.5532520616319444,\n","  0.5749059244791667,\n","  0.5778255208333333,\n","  0.5855107421875],\n"," 'train_loss': [2.7657034595807395,\n","  2.762599309285482,\n","  2.7591700037320455,\n","  2.760571996370951,\n","  2.7619816144307454,\n","  2.747318454583486,\n","  2.724489180246989,\n","  2.7403605540593463,\n","  2.721375068028768,\n","  2.716049305597941],\n"," 'val_loss': [2.8513951460520426,\n","  2.819278939565023,\n","  2.813003746668498,\n","  2.8490739186604817,\n","  2.817236057917277,\n","  2.78973077138265,\n","  2.8040228048960367,\n","  2.782199827829997,\n","  2.7795522212982178,\n","  2.7717891693115235]}"]},"metadata":{},"execution_count":23}],"source":["fit(params['epochs'], model, train_loader, val_loader, criterion, optimizer, scheduler, patience)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DLEDnHIxO8iO","outputId":"684acff1-4ed0-4d91-8263-29537c105246","executionInfo":{"status":"ok","timestamp":1707317273785,"user_tz":-60,"elapsed":636,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 240, 320]           9,408\n","       BatchNorm2d-2         [-1, 64, 240, 320]             128\n","              ReLU-3         [-1, 64, 240, 320]               0\n","         MaxPool2d-4         [-1, 64, 120, 160]               0\n","            Conv2d-5         [-1, 64, 120, 160]          36,864\n","       BatchNorm2d-6         [-1, 64, 120, 160]             128\n","              ReLU-7         [-1, 64, 120, 160]               0\n","            Conv2d-8         [-1, 64, 120, 160]          36,864\n","       BatchNorm2d-9         [-1, 64, 120, 160]             128\n","             ReLU-10         [-1, 64, 120, 160]               0\n","       BasicBlock-11         [-1, 64, 120, 160]               0\n","           Conv2d-12         [-1, 64, 120, 160]          36,864\n","      BatchNorm2d-13         [-1, 64, 120, 160]             128\n","             ReLU-14         [-1, 64, 120, 160]               0\n","           Conv2d-15         [-1, 64, 120, 160]          36,864\n","      BatchNorm2d-16         [-1, 64, 120, 160]             128\n","             ReLU-17         [-1, 64, 120, 160]               0\n","       BasicBlock-18         [-1, 64, 120, 160]               0\n","           Conv2d-19         [-1, 64, 120, 160]          36,864\n","      BatchNorm2d-20         [-1, 64, 120, 160]             128\n","             ReLU-21         [-1, 64, 120, 160]               0\n","           Conv2d-22         [-1, 64, 120, 160]          36,864\n","      BatchNorm2d-23         [-1, 64, 120, 160]             128\n","             ReLU-24         [-1, 64, 120, 160]               0\n","       BasicBlock-25         [-1, 64, 120, 160]               0\n","           Conv2d-26          [-1, 128, 60, 80]          73,728\n","      BatchNorm2d-27          [-1, 128, 60, 80]             256\n","             ReLU-28          [-1, 128, 60, 80]               0\n","           Conv2d-29          [-1, 128, 60, 80]         147,456\n","      BatchNorm2d-30          [-1, 128, 60, 80]             256\n","           Conv2d-31          [-1, 128, 60, 80]           8,192\n","      BatchNorm2d-32          [-1, 128, 60, 80]             256\n","             ReLU-33          [-1, 128, 60, 80]               0\n","       BasicBlock-34          [-1, 128, 60, 80]               0\n","           Conv2d-35          [-1, 128, 60, 80]         147,456\n","      BatchNorm2d-36          [-1, 128, 60, 80]             256\n","             ReLU-37          [-1, 128, 60, 80]               0\n","           Conv2d-38          [-1, 128, 60, 80]         147,456\n","      BatchNorm2d-39          [-1, 128, 60, 80]             256\n","             ReLU-40          [-1, 128, 60, 80]               0\n","       BasicBlock-41          [-1, 128, 60, 80]               0\n","           Conv2d-42          [-1, 128, 60, 80]         147,456\n","      BatchNorm2d-43          [-1, 128, 60, 80]             256\n","             ReLU-44          [-1, 128, 60, 80]               0\n","           Conv2d-45          [-1, 128, 60, 80]         147,456\n","      BatchNorm2d-46          [-1, 128, 60, 80]             256\n","             ReLU-47          [-1, 128, 60, 80]               0\n","       BasicBlock-48          [-1, 128, 60, 80]               0\n","           Conv2d-49          [-1, 128, 60, 80]         147,456\n","      BatchNorm2d-50          [-1, 128, 60, 80]             256\n","             ReLU-51          [-1, 128, 60, 80]               0\n","           Conv2d-52          [-1, 128, 60, 80]         147,456\n","      BatchNorm2d-53          [-1, 128, 60, 80]             256\n","             ReLU-54          [-1, 128, 60, 80]               0\n","       BasicBlock-55          [-1, 128, 60, 80]               0\n","    ResNetEncoder-56  [[-1, 3, 480, 640], [-1, 64, 240, 320], [-1, 64, 120, 160], [-1, 128, 60, 80]]               0\n","AdaptiveAvgPool2d-57            [-1, 128, 1, 1]               0\n","           Conv2d-58             [-1, 32, 1, 1]           4,128\n","         Identity-59             [-1, 32, 1, 1]               0\n","             ReLU-60             [-1, 32, 1, 1]               0\n","         PSPBlock-61           [-1, 32, 60, 80]               0\n","AdaptiveAvgPool2d-62            [-1, 128, 2, 2]               0\n","           Conv2d-63             [-1, 32, 2, 2]           4,096\n","      BatchNorm2d-64             [-1, 32, 2, 2]              64\n","             ReLU-65             [-1, 32, 2, 2]               0\n","         PSPBlock-66           [-1, 32, 60, 80]               0\n","AdaptiveAvgPool2d-67            [-1, 128, 3, 3]               0\n","           Conv2d-68             [-1, 32, 3, 3]           4,096\n","      BatchNorm2d-69             [-1, 32, 3, 3]              64\n","             ReLU-70             [-1, 32, 3, 3]               0\n","         PSPBlock-71           [-1, 32, 60, 80]               0\n","AdaptiveAvgPool2d-72            [-1, 128, 6, 6]               0\n","           Conv2d-73             [-1, 32, 6, 6]           4,096\n","      BatchNorm2d-74             [-1, 32, 6, 6]              64\n","             ReLU-75             [-1, 32, 6, 6]               0\n","         PSPBlock-76           [-1, 32, 60, 80]               0\n","        PSPModule-77          [-1, 256, 60, 80]               0\n","           Conv2d-78          [-1, 512, 60, 80]         131,072\n","      BatchNorm2d-79          [-1, 512, 60, 80]           1,024\n","             ReLU-80          [-1, 512, 60, 80]               0\n","        Dropout2d-81          [-1, 512, 60, 80]               0\n","       PSPDecoder-82          [-1, 512, 60, 80]               0\n","           Conv2d-83           [-1, 27, 60, 80]         124,443\n","UpsamplingBilinear2d-84         [-1, 27, 480, 640]               0\n","          Softmax-85         [-1, 27, 480, 640]               0\n","       Activation-86         [-1, 27, 480, 640]               0\n","================================================================\n","Total params: 1,621,051\n","Trainable params: 1,621,051\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 3.52\n","Forward/backward pass size (MB): 40407052320009.89\n","Params size (MB): 6.18\n","Estimated Total Size (MB): 40407052320019.59\n","----------------------------------------------------------------\n"]}],"source":["torchsummary.summary(model, (3,480,640))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"LhRpdv4y9Aqt","executionInfo":{"status":"ok","timestamp":1707317285936,"user_tz":-60,"elapsed":483,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["class Hurricane_Harvey_inference(Dataset):\n","    def __init__(self, images_filenames, images_directory, transform=None):\n","        self.images_filenames = images_filenames\n","        self.images_directory = images_directory\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images_filenames)\n","\n","    def __getitem__(self, idx):\n","        image_filename = self.images_filenames[idx]\n","        image_path = (os.path.join(self.images_directory, image_filename) + \".tif\")\n","        image = cv2.imread(image_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        original_size = tuple(image.shape[:2])\n","        #image = cv2.resize(image, (256, 256))\n","        if self.transform is not None:\n","            transformed = self.transform(image=image)\n","            image = transformed[\"image\"]\n","        #print(type(image))\n","        return image, original_size, image_filename"]},{"cell_type":"markdown","metadata":{"id":"Eq-GXUTwudnu"},"source":["# Testing the model"]},{"cell_type":"markdown","metadata":{"id":"bcL9hbnCu282"},"source":["Here we run our transformations on the test pics instead"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"ll6ctm-08Rvd","executionInfo":{"status":"ok","timestamp":1707317288245,"user_tz":-60,"elapsed":6,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["test_transform = A.Compose(\n","    [\n","        A.Resize(480, 640),\n","        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","        ToTensorV2(),\n","    ]\n",")\n","test_dataset = Hurricane_Harvey_inference(test_pics, raw_folder_path, transform=test_transform,)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"rS-FJaFtwDFQ","executionInfo":{"status":"ok","timestamp":1707317291387,"user_tz":-60,"elapsed":5,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["def predict(model, params, test_dataset, batch_size):\n","    test_loader = DataLoader(\n","        test_dataset, batch_size=batch_size, shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n","    )\n","    model.eval()\n","    predictions = []\n","    with torch.no_grad():\n","        for images, (original_heights, original_widths), image_filename in test_loader:\n","            images = images.to(device, non_blocking=True)\n","            output = model(images)\n","            #print(len(torch.unique(output)))\n","            probabilities = torch.argmax(output, dim=1)\n","            #print(torch.unique(probabilities)) # to see if the output of probabilites is composed by different numbers\n","            predicted_masks = probabilities.cpu().squeeze(0)\n","            predicted_masks = predicted_masks.cpu().numpy().astype(np.uint8)\n","            i = 0\n","            for predicted_mask, original_height, original_width in zip(\n","                predicted_masks, original_heights.numpy(), original_widths.numpy()\n","            ):\n","                predictions.append((predicted_mask, original_height, original_width, image_filename[i]))\n","                i = i + 1\n","    return predictions"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"HWHaHIwf8k46","executionInfo":{"status":"ok","timestamp":1707317320018,"user_tz":-60,"elapsed":22494,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["predictions = predict(model, params, test_dataset, batch_size=4)\n","# the printed output you see here is to see if the probabilietis variable in\n","# predict funciton is producing tensors with different values\n","# in this case is producing tensors with only 0s in them"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"GKTtxOin0G71","executionInfo":{"status":"ok","timestamp":1707317387669,"user_tz":-60,"elapsed":67661,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["from PIL import Image\n","import os\n","from torchvision.transforms import functional as TF\n","\n","# Create folder to save masks\n","if not os.path.exists(path_pred):\n","    os.mkdir(path_pred)\n","\n","# Iterate through predictions\n","for (predicted_mask, original_height, original_width, image_filename) in predictions:\n","    # Resize the mask to the original dimensions\n","    predicted_mask = cv2.resize(predicted_mask, (original_width, original_height), interpolation = cv2.INTER_NEAREST)\n","    # Convert mask to PIL image\n","    predicted_mask = Image.fromarray(predicted_mask)\n","    predicted_mask = predicted_mask.convert(\"L\")\n","    # Save mask to folder\n","    save_path = os.path.join(path_pred, f\"{image_filename}.png\")\n","    #plt.imsave(save_path, predicted_mask, cmap = \"gray\")\n","    predicted_mask.save(save_path, \"PNG\")\n","    #predicted_mask.save(path_pred\"masks/mask_{}.png\".format(i))"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"0SVRpb6lzkz7","executionInfo":{"status":"ok","timestamp":1707317729391,"user_tz":-60,"elapsed":475,"user":{"displayName":"Leonardo Basili","userId":"13408618737026946878"}}},"outputs":[],"source":["# tar submission\n","import tarfile\n","\n","tar = tarfile.open(\"submissionPSPNet5.tar\", \"w\")\n","\n","for root, dir, files in os.walk(path_pred):\n","    for  file in files:\n","        fullpath = os.path.join(root, file)\n","        tar.add(fullpath, arcname=file)\n","\n","tar.close()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
